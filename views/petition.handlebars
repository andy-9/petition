<div class="navbar"><a href="/profile/edit">Edit Profile</a>  |  <a href="/logout">Logout</a></div>
<div class="placeholder"></div>
<div class="placeholder"></div>

<p>This petition is to support the <a href="https://algorules.org/en/home" target="_blank">9 Algo.Rules</a>, a joint statement developed by over 400 people and coordinated by the Bertelsmann Foundation and the think tank iRights.Lab. The key questions behind this statement are whether tools and algorithms support justice and equality and how to make technology great for all humans. By signing this petition you declare your full support for the following rules and demand that every organization, institution, team and individual that deals with algorithms should adhere to them.</p>

<h3>The 9 Algo.Rules are:</h3>

<p><b>1. Strengthen competency</b><br>
The function and potential effects of an algorithmic system must be understood.</p>
<p><b>2. Define responsibilities</b><br>
A natural or legal person must always be held responsible for the effects involved with the use of an algorithmic system.</p>
<p><b>3. Document goals and anticipated impact</b><br>
The objectives and expected impact of the use of an algorithmic system must be documented and assessed prior to implementation.</p>
<p><b>4. Guarantee security</b><br>
The security of an algorithmic system must be tested before and during its implementation.</p>
<p><b>5. Provide labeling</b><br>
The use of an algorithmic system must be identified as such.</p>
<p><b>6. Ensure intelligibility</b><br>
The decision-making processes within an algorithmic system must always be comprehensible.</p>
<p><b>7. Safeguard manageability</b><br>
An algorithmic system must be manageable throughout the lifetime of its use.</p>
<p><b>8. Monitor impact</b><br>
The effects of an algorithmic system must be reviewed on a regular basis.</p>
<p><b>9. Establish complaint mechanisms</b><br>
If an algorithmic system results in a questionable decision or a decision that affects an individualâ€™s rights, it must be possible to request an explanation and file a complaint.</p>
<div class="placeholder"></div>
<p>The long version can be read <a class="hyperlinkblue" href="https://algorules.org/en/home" target="_blank">here</a>.</p>

<h3>Background</h3>
<p>The past decade is characterized by an expansion in the use of algorithms in major spheres of life. The bundling of big data and increased computer power together with advances in data availability and algorithms affect society, politics, institutions, and human behavior in ways not known before. Algorithms predict what you might like, would want to see and listen to, might want to buy, your search engine results, if you are likely to commit a crime, should get a credit or visa or if you are suited to do a job. They direct us to our destination, sort images, predict the weather, detect cancer, drive cars, search targets in war, serve as virtual assistants and translation devices.</p>
<p>Algorithms are usually seen as a neutral technology and thus unbiased. This is true to a certain extent: algorithms do not make arbitrary decisions and they are faster and more accurate than humans.</p>
<p>Yet, the reality looks different. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, individual or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design. Some examples may illustrate this:</p>

<ul class="bulletpoints" type="disc">
    <li>Amazon's hiring process not too long ago singled out women because it was fed with data of the past 10 years where almost exclusively only men got hired.</li>
    <li>The Federal Office for Migration and Refugees in Germany uses software for speech analysis to detect 'wrong' information given by asylum seekers from Arabic countries. Cases got known where people got deported who were not even in the database.</li>
    <li>Nudity filters on instagram filter out people based on the percentage of nudity. Plus-sized people get way easier caught up than thin people.</li>
    <li>The Austrian Public Employment Service uses an algorithm to predict long-term unemployment and potential job placement. The algorithm disproportionately singles out old people, people with disabilities and women with children.</li>
    <li>Some face recognition systems do not recognize Black and Asian women.</li>
</ul>

<p>The examples are many. Algorithmic bias describes systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one arbitrary group of users over others. Bias can emerge due to many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. Many times, historically derived datasets set white, straight, able-bodied, middle- and upper-class cis-men as the norm. If those data from the past are meant to predict the future, all those who are not part of this group are likely to be confronted with bias.</p>

<p><b>To counter algorithmic bias and to support the Algo.Rules please sign the petition and spread the word!</b></p>

<h3>Sign here to support our cause</h3>

<form method="POST" id='clicks'>
    <div>
        <canvas id="canvas" width="700px" height="100px"></canvas>
        <p id="signhere">your signature</p>
        <input id="signature" name="signature" value="" type="hidden"/>
        <input type="hidden" name="_csrf" value="{{csrfToken}}">
    </div>

{{#if error}}
    <h4>Something went wrong. Please sign.</h4>
{{/if}}

    <div id="canvasbuttons">
        <button id="clear" type="button">clear and redraw</button>
        <button id="submit-btn">Sign up</button>
    </div>

</form>

<div class="navbar"><a href="/profile/edit">Edit Profile</a>  |  <a href="/logout">Logout</a></div>

<div class="credits">
    <b>Inspired by: </b><br>
        <a href="https://weneedtotalk.ai/" target="_blank">"We need to talk, AI"</a> by Dr. Julia Schneider & Lena Kadriye Ziyal,
        <a href="https://algorules.org/en/home" target="_blank">Algo.Rules</a>,
        <a href="https://nushinyazdani.com/" target="_blank">Nushin Isabelle Yazdani</a>,
        <a href="https://www.nakeema.net/" target="_blank">Nakeema Stefflbauer</a> and
        <a href="https://algorithmwatch.org/en/team/lorenz-matzat/" target="_blank">Lorenz Matzat</a>
</div>